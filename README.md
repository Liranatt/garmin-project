# Garmin Health Intelligence üèÉ‚Äç‚ôÇÔ∏èüìä

A personal data analytics project designed to dig deeper into wearable health metrics. The goal: take raw Garmin data, apply rigorous statistical analysis, and use specialized AI agents to interpret the results into actionable insights.

## Why I Built This

Garmin Connect is excellent for tracking *what* happened today or this week, but I often felt it lacked the depth to explain *why*. I found myself asking questions that the standard dashboard couldn't answer:

* Does yesterday's high stress *actually* impact my sleep quality tonight?
* Is this drop in HRV just random noise, or the start of a negative trend?
* What is the strongest statistical predictor for a high-performance training day?

Instead of feeding raw CSV files into an LLM and hoping for the best (which often leads to hallucinations), I built a pipeline that separates **mathematical calculation** from **AI interpretation**.

## How It Works

The system runs as a weekly pipeline:

1.  **Data Collection (`src/enhanced_fetcher.py`)**
    Fetches raw data from the Garmin API (using `garth`) and upserts it into a PostgreSQL database. It handles API instability with robust retry logic.

2.  **Correlation Engine (`src/correlation_engine.py`)**
    The heart of the system. Before any AI touches the data, this engine calculates:
    * **Pearson Correlations:** Identifying linear relationships between metrics.
    * **AR(1) Models:** Analyzing the "momentum" or persistence of specific metrics.
    * **Markov Transitions:** Calculating the probability of moving from one state to another (e.g., probability of a "High Recovery" day following a "Poor Sleep" night).

3.  **AI Agents (`src/enhanced_agents.py`)**
    A team of 9 specialized agents (built with CrewAI & Gemini 2.5 Flash). They don't analyze the raw numbers directly; instead, they receive the *statistical summary* generated by the engine. This ensures their insights are grounded in math, not guesses.

4.  **Dashboard (`src/dashboard.py`)**
    A Streamlit interface to visualize trends, explore the data, and chat with the agents.

## Technical Challenges & Solutions

Building this revealed some interesting data engineering hurdles:

### The "Date Gap" Problem
I discovered that standard data analysis functions like `.shift(1)` are dangerous with wearable data. If I don't wear the watch for a weekend, a naive shift would compare Friday's data with Monday's as if they were consecutive days, corrupting the correlation analysis.
**Solution:** I implemented strict date enforcement (`.asfreq('D')`) to insert `NaN` rows for missing days, ensuring the statistical models only analyze true consecutive sequences.

### preventing AI Hallucinations
LLMs are great at sounding confident but bad at math. To prevent "hallucinated correlations," the AI never sees the raw CSV. It only receives a "Context Window" ‚Äî a structured text summary generated by the Correlation Engine *after* statistical significance has been proven ($p < 0.05$).

### Signal vs. Noise
Wearable data is noisy. To make the Markov transition matrices useful with limited data, I implemented Adaptive Kernel Smoothing. This prevents a single outlier day from skewing the entire probability model.

## Tech Stack

* **Language:** Python 3.12
* **Database:** PostgreSQL (Heroku)
* **AI Framework:** CrewAI + Gemini 2.5 Flash
* **Visualization:** Streamlit + Plotly
* **Libraries:** NumPy, SciPy, Pandas, Tenacity

## The Agents

The system runs several "experts" in parallel:
* **Matrix Analyst:** Interprets the correlation matrices.
* **Pattern Detective:** Looks for non-linear hidden patterns.
* **Sleep Analyst:** Focuses specifically on sleep architecture and consistency.
* **Recovery Specialist:** Analyzes bounce-back speed and physiological cost.

---

*This project is for educational and personal use, exploring the intersection of Data Engineering, Statistics, and GenAI.*
